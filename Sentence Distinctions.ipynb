{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='pink'>Sentence Distinctions</font>\n",
    "\n",
    "By: Ameer Syedibrahim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syediam\\AppData\\Local\\Cisco\\New\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time, math, nltk, gensim, string\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in Word Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Elapsed 22.985545873641968s\n"
     ]
    }
   ],
   "source": [
    "path = \"glove.6B.50d.txt.w2v\"\n",
    "t0 = time.time()\n",
    "glove = KeyedVectors.load_word2vec_format(path, binary=False)\n",
    "t1 = time.time()\n",
    "print(\"Time Elapsed: \" + str(t1 - t0) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a word's numerical, conceptually-informed representation\n",
    "In this example, this vector of 50 numbers represents the word 'pink'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PINK\n",
      "[ -1.72710001e-01   9.54169989e-01  -8.44640017e-01   1.18179999e-01\n",
      "   8.76410007e-02   8.43370020e-01  -7.33470023e-01  -1.10930002e+00\n",
      "  -2.56430000e-01   9.72340032e-02   1.29079996e-02   1.37009993e-01\n",
      "   7.26750016e-01   3.31180006e-01   2.56839991e-01  -2.03779992e-02\n",
      "  -5.47829986e-01  -6.91029988e-03  -3.81839991e-01  -1.36829996e+00\n",
      "  -8.07910025e-01  -2.22609997e-01   6.60849988e-01  -6.94389999e-01\n",
      "  -9.76220012e-01  -2.33370006e-01  -1.06280005e+00   2.00449991e+00\n",
      "   4.27769989e-01  -1.44570005e+00   1.62930000e+00   7.37240016e-02\n",
      "  -3.53700012e-01   5.58449984e-01  -2.82240003e-01   3.14090014e-01\n",
      "   2.52099991e-01  -3.71149987e-01  -1.98949993e-01  -1.25520003e+00\n",
      "   1.08089998e-01   8.82899985e-02  -4.71709996e-01  -7.04760015e-01\n",
      "   8.46409976e-01  -4.31089997e-01   1.90270005e-03  -1.71399999e+00\n",
      "  -1.19319998e-01  -5.12709975e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"PINK\")\n",
    "print(glove.wv[\"pink\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160000000\n"
     ]
    }
   ],
   "source": [
    "n = len(glove.vocab)\n",
    "d = glove.vector_size\n",
    "X_glove = np.zeros((n, d))\n",
    "for i, word in enumerate(glove.vocab.keys()):\n",
    "    X_glove[i,:] = glove.wv[word]\n",
    "print(X_glove.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed 1.9745051860809326s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(X_glove)\n",
    "t1 = time.time()\n",
    "print(\"elapsed \" + str(t1 - t0) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Relation Calculator\n",
    "If the outputted number is less that 14, the model recognizes them as related. If the number is greater than 14, then the model recognizes them as unrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def the_sent_threshold():\n",
    "    \n",
    "    str1 = input(\"Enter the first sentence: \")\n",
    "    \n",
    "    list1 = nltk.word_tokenize(str1)\n",
    "    \n",
    "    vector1list = []\n",
    "    \n",
    "    for word in list1:\n",
    "        \n",
    "        vector1list.append(glove.wv[word])\n",
    "        \n",
    "        \n",
    "    vector1_array = np.array(vector1list)\n",
    "    \n",
    "    str2 = input(\"Enter the second sentence: \")\n",
    "    \n",
    "    list2 = nltk.word_tokenize(str2)\n",
    "    \n",
    "    vector2list = []\n",
    "    \n",
    "    for word in list2:\n",
    "        \n",
    "        vector2list.append(glove.wv[word])\n",
    "        \n",
    "        \n",
    "    vector2_array = np.array(vector2list)\n",
    "    \n",
    "    diff = vector1_array - vector2_array\n",
    "    \n",
    "    \n",
    "    \n",
    "    thresh = math.sqrt(abs(np.sum(diff))*21)\n",
    "    \n",
    "    \n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Sentence Relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_sent_threshold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Filtration of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(text, preserve_case=True, filter_text=True, stopwords=None, punc=string.punctuation):\n",
    "    if filter_text and stopwords is None:\n",
    "        with open(\"stopwords.txt\", 'r') as r:\n",
    "            stops = []\n",
    "            for line in r:\n",
    "                stops += [i.strip() for i in line.split('\\t')]\n",
    "    stopwords = stops\n",
    "    tokens = word_tokenize(text)\n",
    "    indices = []\n",
    "    if filter_text:\n",
    "        for i in range(len(tokens)):\n",
    "            word = tokens[i]\n",
    "            # print(word, word in punc, word.lower() in stopwords)\n",
    "            if word in punc or word.lower() in stopwords:\n",
    "                indices.append(i)\n",
    "        tokens = list(np.delete(np.array(tokens), np.array(indices)))\n",
    "    if preserve_case:\n",
    "        return tokens\n",
    "    return [i.lower() for i in a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Relation Calculator 2.0\n",
    "An improved version of the sentence relation calculator that can accept sentences with different amount of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_threshold1():\n",
    "    \n",
    "    str1 = input(\"Enter the first sentence: \")\n",
    "    \n",
    "    list1 = tokenize(str1)\n",
    "    \n",
    "    vector1list = []\n",
    "    \n",
    "    for word in list1:\n",
    "        \n",
    "        vector1list.append(glove.wv[word])\n",
    "        \n",
    "        \n",
    "    vector1_array = np.array(vector1list)\n",
    "    \n",
    "    str2 = input(\"Enter the second sentence: \")\n",
    "    \n",
    "    list2 = tokenize(str2)\n",
    "    \n",
    "    vector2list = []\n",
    "    \n",
    "    for word in list2:\n",
    "        \n",
    "        vector2list.append(glove.wv[word])\n",
    "        \n",
    "        \n",
    "    vector2_array = np.array(vector2list)\n",
    "    \n",
    "    \n",
    "    dim1 = vector1_array.shape[0]\n",
    "    dim2 = vector2_array.shape[0]\n",
    "    \n",
    "    if dim1 > dim2 :\n",
    "        \n",
    "        diff = dim1 - dim2\n",
    "        \n",
    "        for i in range(diff):\n",
    "            arr = np.mean(vector2_array,axis=0)\n",
    "            \n",
    "            vector2_array = np.vstack([vector2_array,arr])\n",
    "            \n",
    "    if dim2 > dim1 :\n",
    "        \n",
    "        diff = dim2 - dim1\n",
    "        \n",
    "        for i in range(diff):\n",
    "            arr = np.mean(vector1_array,axis=0)\n",
    "            \n",
    "            vector1_array = np.vstack([vector1_array,arr])\n",
    "            \n",
    "    \n",
    "    diff = vector1_array - vector2_array\n",
    "    \n",
    "    \n",
    "    \n",
    "    thresh = math.sqrt(abs(np.sum(diff))*21)\n",
    "    \n",
    "    \n",
    "    return thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Sentence Relations 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_threshold1()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
